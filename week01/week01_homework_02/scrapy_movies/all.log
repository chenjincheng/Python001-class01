2020-06-30 23:57:37 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-06-30 23:57:37 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-06-30 23:57:37 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-06-30 23:57:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-06-30 23:57:37 [scrapy.extensions.telnet] INFO: Telnet Password: eb0110ebf77c73fb
2020-06-30 23:57:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-06-30 23:57:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-06-30 23:57:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-06-30 23:57:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-06-30 23:57:38 [scrapy.core.engine] INFO: Spider opened
2020-06-30 23:57:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-06-30 23:57:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-06-30 23:57:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-06-30 23:57:38 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=0bec70e7ffa84d2fb16a32562ac92264&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-06-30 23:57:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-06-30 23:57:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=0bec70e7ffa84d2fb16a32562ac92264&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-06-30 23:57:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=0bec70e7ffa84d2fb16a32562ac92264&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 22, in parse
    file = open('./index.html', 'r')
FileNotFoundError: [Errno 2] No such file or directory: './index.html'
2020-06-30 23:57:39 [scrapy.core.engine] INFO: Closing spider (finished)
2020-06-30 23:57:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17124,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.948639,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 15, 57, 39, 186452),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/FileNotFoundError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 15, 57, 38, 237813)}
2020-06-30 23:57:39 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:00:12 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:00:12 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:00:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:00:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:00:12 [scrapy.extensions.telnet] INFO: Telnet Password: 8db5b1c54daea418
2020-07-01 00:00:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:00:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:00:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:00:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:00:13 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:00:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:00:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:00:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:00:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=1d106b82e47d44abb81351fb1c61c62a&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:00:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:00:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=1d106b82e47d44abb81351fb1c61c62a&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:00:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=1d106b82e47d44abb81351fb1c61c62a&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:00:14 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:00:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17129,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.977363,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 0, 14, 744078),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 0, 13, 766715)}
2020-07-01 00:00:14 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:01:24 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:01:24 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:01:24 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:01:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:01:24 [scrapy.extensions.telnet] INFO: Telnet Password: 0a5bcc6288da72a4
2020-07-01 00:01:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:01:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:01:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:01:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:01:25 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:01:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:01:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:01:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:01:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=f0180c9066fb41b38eba2830dc0b5387&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:01:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:01:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=f0180c9066fb41b38eba2830dc0b5387&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:01:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=f0180c9066fb41b38eba2830dc0b5387&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:01:25 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:01:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17129,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.917515,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 1, 25, 969516),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 1, 25, 52001)}
2020-07-01 00:01:25 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:02:13 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:02:13 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:02:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:02:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:02:13 [scrapy.extensions.telnet] INFO: Telnet Password: 4cec29c0d44eb191
2020-07-01 00:02:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:02:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:02:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:02:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:02:14 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:02:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:02:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:02:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:02:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=a72901eecdd740129a99f6823ae2c3e7&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:02:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:02:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=a72901eecdd740129a99f6823ae2c3e7&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:02:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=a72901eecdd740129a99f6823ae2c3e7&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:02:15 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:02:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17124,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.889681,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 2, 15, 753370),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 2, 14, 863689)}
2020-07-01 00:02:15 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:02:55 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:02:55 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:02:55 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:02:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:02:55 [scrapy.extensions.telnet] INFO: Telnet Password: dd692c3d5a6b65ec
2020-07-01 00:02:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:02:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:02:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:02:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:02:56 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:02:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:02:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:02:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:02:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=de9989010c3d4cb18e80d14c3da92aaa&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:02:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:02:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=de9989010c3d4cb18e80d14c3da92aaa&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:02:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=de9989010c3d4cb18e80d14c3da92aaa&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:02:57 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:02:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17122,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.893602,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 2, 57, 112686),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 2, 56, 219084)}
2020-07-01 00:02:57 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:03:04 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:03:04 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:03:04 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:03:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:03:04 [scrapy.extensions.telnet] INFO: Telnet Password: 1aa59cd56ba88a01
2020-07-01 00:03:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:03:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:03:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:03:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:03:05 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:03:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:03:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:03:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:03:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=b3aae76a9ad548adbf0325258256866f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:03:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:03:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=b3aae76a9ad548adbf0325258256866f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:03:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=b3aae76a9ad548adbf0325258256866f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:03:06 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:03:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17120,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.974385,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 3, 6, 340997),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 3, 5, 366612)}
2020-07-01 00:03:06 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:03:51 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:03:51 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:03:51 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:03:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:03:51 [scrapy.extensions.telnet] INFO: Telnet Password: ad0071208b75dcc6
2020-07-01 00:03:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:03:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:03:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:03:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:03:52 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:03:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:03:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:03:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:03:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=d3d11215d3e04cafac796ad8674fac86&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:03:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:03:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=d3d11215d3e04cafac796ad8674fac86&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:03:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=d3d11215d3e04cafac796ad8674fac86&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 22, in parse
    file = open('./week01/week01_homework_02/scrapy_movies/index.html', 'r')
FileNotFoundError: [Errno 2] No such file or directory: './week01/week01_homework_02/scrapy_movies/index.html'
2020-07-01 00:03:53 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:03:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17127,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.909854,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 3, 53, 82239),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/FileNotFoundError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 3, 52, 172385)}
2020-07-01 00:03:53 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:04:41 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:04:41 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:04:41 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:04:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:04:41 [scrapy.extensions.telnet] INFO: Telnet Password: 4e6fe1179c1c4ff8
2020-07-01 00:04:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:04:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:04:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:04:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:04:42 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:04:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:04:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:04:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:04:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=59825c9792db4368a809d3534a27c7ad&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:04:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:04:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=59825c9792db4368a809d3534a27c7ad&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:04:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=59825c9792db4368a809d3534a27c7ad&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:04:43 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:04:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17124,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.875003,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 4, 43, 378711),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 4, 42, 503708)}
2020-07-01 00:04:43 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:05:00 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:05:00 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:05:00 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:05:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:05:00 [scrapy.extensions.telnet] INFO: Telnet Password: 269deb5f097f5a72
2020-07-01 00:05:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:05:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:05:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:05:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:05:01 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:05:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:05:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:05:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:05:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=3a445f64cd6a43259a539158cd0f31e9&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:05:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:05:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=3a445f64cd6a43259a539158cd0f31e9&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:05:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=3a445f64cd6a43259a539158cd0f31e9&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:05:02 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:05:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17122,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.872834,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 5, 2, 729751),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 5, 1, 856917)}
2020-07-01 00:05:02 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:05:41 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:05:41 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:05:41 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:05:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:05:41 [scrapy.extensions.telnet] INFO: Telnet Password: 58c33c917b66bd6e
2020-07-01 00:05:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:05:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:05:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:05:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:05:42 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:05:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:05:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:05:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:05:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=e727ab2e01e04aafa2fe576e57e0804f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:05:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:05:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=e727ab2e01e04aafa2fe576e57e0804f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:05:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=e727ab2e01e04aafa2fe576e57e0804f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:05:43 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:05:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17120,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.927586,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 5, 43, 369933),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 5, 42, 442347)}
2020-07-01 00:05:43 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:06:37 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:06:37 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:06:37 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:06:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:06:37 [scrapy.extensions.telnet] INFO: Telnet Password: d0f206d450add059
2020-07-01 00:06:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:06:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:06:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:06:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:06:38 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:06:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:06:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:06:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:06:38 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=75b7f793dd3743108e902187eb0b3cf1&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:06:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:06:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=75b7f793dd3743108e902187eb0b3cf1&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:06:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=75b7f793dd3743108e902187eb0b3cf1&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:06:39 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:06:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17123,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.893621,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 6, 39, 557526),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 6, 38, 663905)}
2020-07-01 00:06:39 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:07:04 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:07:04 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:07:04 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:07:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:07:04 [scrapy.extensions.telnet] INFO: Telnet Password: 6be0d3dd18ee696f
2020-07-01 00:07:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:07:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:07:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:07:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:07:05 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:07:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:07:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:07:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:07:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=41a9ed673c7349319abed8d0e7be582c&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:07:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:07:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=41a9ed673c7349319abed8d0e7be582c&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:07:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=41a9ed673c7349319abed8d0e7be582c&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    text = file.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:07:06 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:07:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17124,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.910654,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 7, 6, 478576),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 7, 5, 567922)}
2020-07-01 00:07:06 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:07:27 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:07:27 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:07:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:07:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:07:27 [scrapy.extensions.telnet] INFO: Telnet Password: cab00e79aa4e9e06
2020-07-01 00:07:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:07:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:07:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:07:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:07:28 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:07:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:07:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:07:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:07:28 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=a83627e47bbb4e5b83d5fffd806e8396&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:07:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:07:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=a83627e47bbb4e5b83d5fffd806e8396&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:07:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=a83627e47bbb4e5b83d5fffd806e8396&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    text = file.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:07:28 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:07:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17124,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.923484,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 7, 28, 944354),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 7, 28, 20870)}
2020-07-01 00:07:28 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:07:54 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:07:54 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:07:54 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:07:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:07:54 [scrapy.extensions.telnet] INFO: Telnet Password: fe06d7ade384ab1e
2020-07-01 00:07:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:07:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:07:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:07:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:07:55 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:07:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:07:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:07:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:07:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=28286a3bce9a4c119ae888dde811a13f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:07:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:07:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=28286a3bce9a4c119ae888dde811a13f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=28286a3bce9a4c119ae888dde811a13f&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    text = file.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:07:56 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:07:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17125,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.939443,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 7, 56, 661193),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 7, 55, 721750)}
2020-07-01 00:07:56 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:08:49 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:08:49 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:08:49 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:08:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:08:50 [scrapy.extensions.telnet] INFO: Telnet Password: 483bdcd3b01563c4
2020-07-01 00:08:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:08:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:08:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:08:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:08:50 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:08:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:08:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:08:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:08:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=2ed322fa55f44d52ae3f008e1884528c&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:08:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:08:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=2ed322fa55f44d52ae3f008e1884528c&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:08:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=2ed322fa55f44d52ae3f008e1884528c&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    text = file.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:08:51 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:08:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17127,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.920575,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 8, 51, 840596),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 8, 50, 920021)}
2020-07-01 00:08:51 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:15:22 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:15:22 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:15:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:15:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:15:22 [scrapy.extensions.telnet] INFO: Telnet Password: 114d3f5946ac8d44
2020-07-01 00:15:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:15:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:15:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:15:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:15:23 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:15:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:15:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:15:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:15:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=f79e640de456444aa71e51dfe952a0bd&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:15:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:15:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=f79e640de456444aa71e51dfe952a0bd&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:15:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=f79e640de456444aa71e51dfe952a0bd&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    text = file.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:15:24 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:15:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17126,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.97044,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 15, 24, 876389),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 15, 23, 905949)}
2020-07-01 00:15:24 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:15:59 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:15:59 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:15:59 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:15:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:15:59 [scrapy.extensions.telnet] INFO: Telnet Password: cc14943ea09fce30
2020-07-01 00:15:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:16:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:16:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:16:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:16:00 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:16:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:16:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:16:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:16:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=41830bb5ebb74cf99ae2679b06399382&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:16:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:16:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=41830bb5ebb74cf99ae2679b06399382&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:16:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=41830bb5ebb74cf99ae2679b06399382&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    text = file.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0xab in position 221: illegal multibyte sequence
2020-07-01 00:16:01 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:16:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17124,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.951454,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 16, 1, 739715),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 16, 0, 788261)}
2020-07-01 00:16:01 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:17:02 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:17:02 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:17:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:17:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:17:02 [scrapy.extensions.telnet] INFO: Telnet Password: f9a7c9027c15f482
2020-07-01 00:17:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:17:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:17:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:17:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:17:03 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:17:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:17:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:17:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:17:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=4eb92ea02464489eb66327161ccc1780&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:17:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:17:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=4eb92ea02464489eb66327161ccc1780&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:17:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=4eb92ea02464489eb66327161ccc1780&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 28, in parse
    print(text == None)
UnboundLocalError: local variable 'text' referenced before assignment
2020-07-01 00:17:04 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:17:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17128,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.903871,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 17, 4, 44346),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnboundLocalError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 17, 3, 140475)}
2020-07-01 00:17:04 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:17:22 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:17:22 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:17:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:17:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:17:22 [scrapy.extensions.telnet] INFO: Telnet Password: ac24938be46d8e01
2020-07-01 00:17:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:17:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:17:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:17:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:17:23 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:17:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:17:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:17:23 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=83a5e37ea6e248eb83ed6e8c99a10c06&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=83a5e37ea6e248eb83ed6e8c99a10c06&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:17:24 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:17:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17126,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.796869,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 17, 24, 115361),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 6, 30, 16, 17, 23, 318492)}
2020-07-01 00:17:24 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:18:28 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:18:28 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:18:28 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:18:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:18:28 [scrapy.extensions.telnet] INFO: Telnet Password: 09a0d3d2a2298172
2020-07-01 00:18:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:18:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:18:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:18:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:18:29 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:18:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:18:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:18:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:18:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=9c2f83c67f0e448195059f19de6dd292&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:18:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:18:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=9c2f83c67f0e448195059f19de6dd292&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:18:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=9c2f83c67f0e448195059f19de6dd292&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 21, in parse
    response.text = file = open(r'index.html', 'r', True, encoding='utf-8').read()
AttributeError: can't set attribute
2020-07-01 00:18:30 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:18:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17122,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.929515,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 18, 30, 396977),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 18, 29, 467462)}
2020-07-01 00:18:30 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:18:43 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:18:43 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:18:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:18:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:18:43 [scrapy.extensions.telnet] INFO: Telnet Password: 0855be1ace602c45
2020-07-01 00:18:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:18:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:18:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:18:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:18:43 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:18:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:18:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:18:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=b6a2ce6d369d450b82ace593a9f99f6b&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=b6a2ce6d369d450b82ace593a9f99f6b&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:18:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=b6a2ce6d369d450b82ace593a9f99f6b&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 21, in parse
    response.text = open(r'index.html', 'r', True, encoding='utf-8').read()
AttributeError: can't set attribute
2020-07-01 00:18:44 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:18:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17123,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.915597,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 18, 44, 898223),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 18, 43, 982626)}
2020-07-01 00:18:44 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:19:04 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:19:04 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:19:04 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:19:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:19:04 [scrapy.extensions.telnet] INFO: Telnet Password: dc4d09637bb05199
2020-07-01 00:19:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:19:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:19:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:19:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:19:05 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:19:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:19:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:19:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:19:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=dc617f47ec564b31b85608d8e46743ae&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:19:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:19:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=dc617f47ec564b31b85608d8e46743ae&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:19:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=dc617f47ec564b31b85608d8e46743ae&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 21, in parse
    response.text = open(r'index.html', 'r', True, encoding='utf-8').read()
AttributeError: can't set attribute
2020-07-01 00:19:06 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:19:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17123,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.924601,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 19, 6, 234205),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 19, 5, 309604)}
2020-07-01 00:19:06 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:19:45 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:19:45 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:19:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:19:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:19:45 [scrapy.extensions.telnet] INFO: Telnet Password: e36d9795d2a66738
2020-07-01 00:19:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:19:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:19:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:19:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:19:46 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:19:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:19:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://maoyan.com/robots.txt> (referer: None)
2020-07-01 00:19:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=ac6ba755f66b4f0aa5d8717b96795970&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> from <GET https://maoyan.com/films?showType=3>
2020-07-01 00:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/robots.txt> (referer: None)
2020-07-01 00:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://verify.meituan.com/v2/web/general_page?action=spiderindefence&requestCode=ac6ba755f66b4f0aa5d8717b96795970&platform=1000&adaptor=auto&succCallbackUrl=https%3A%2F%2Foptimus-mtsi.meituan.com%2Foptimus%2FverifyResult%3ForiginUrl%3Dhttp%253A%252F%252Fmaoyan.com%252Ffilms%253FshowType%253D3> (referer: None)
2020-07-01 00:19:47 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:19:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1571,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 17125,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 0.937451,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 19, 47, 844761),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 6, 30, 16, 19, 46, 907310)}
2020-07-01 00:19:47 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:23:07 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:23:07 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:23:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:23:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:23:07 [scrapy.extensions.telnet] INFO: Telnet Password: 86fb8eb7e6a13061
2020-07-01 00:23:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:23:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:23:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:23:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:23:08 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:23:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:23:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:23:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:23:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:23:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:23:08 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:23:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:23:08 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:23:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.37702,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 23, 8, 812865),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 23, 8, 435845)}
2020-07-01 00:23:08 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:25:57 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:25:57 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:25:57 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:25:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:25:58 [scrapy.extensions.telnet] INFO: Telnet Password: 6986879544827b50
2020-07-01 00:25:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:25:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:25:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:25:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:25:58 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:25:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:25:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:25:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:25:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:25:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:25:58 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:25:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:25:59 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:25:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.148599,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 25, 59, 102064),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 25, 58, 953465)}
2020-07-01 00:25:59 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:26:32 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:26:32 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:26:32 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:26:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:26:32 [scrapy.extensions.telnet] INFO: Telnet Password: d9116f226d621018
2020-07-01 00:26:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:26:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:26:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:26:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:26:33 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:26:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:26:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:26:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:26:33 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:26:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.164559,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 26, 33, 178846),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 26, 33, 14287)}
2020-07-01 00:26:33 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:26:44 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:26:44 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:26:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:26:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:26:44 [scrapy.extensions.telnet] INFO: Telnet Password: 7cc5b470159e2c37
2020-07-01 00:26:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:26:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:26:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:26:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:26:45 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:26:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:26:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:26:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:45 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:26:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:26:45 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:26:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.164514,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 26, 45, 244564),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 26, 45, 80050)}
2020-07-01 00:26:45 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:28:55 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:28:55 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:28:55 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:28:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:28:55 [scrapy.extensions.telnet] INFO: Telnet Password: f5ea6c46a1043168
2020-07-01 00:28:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:28:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:28:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:28:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:28:56 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:28:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:28:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:28:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:28:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:28:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:28:56 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:28:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:28:56 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:28:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.171495,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 28, 56, 726777),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 28, 56, 555282)}
2020-07-01 00:28:56 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:29:09 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:29:09 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:29:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:29:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:29:09 [scrapy.extensions.telnet] INFO: Telnet Password: d7a1559a23f1bbe6
2020-07-01 00:29:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:29:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:29:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:29:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:29:11 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:29:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:29:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:29:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:29:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:29:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:29:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:29:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:29:11 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:29:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.149601,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 29, 11, 207035),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 29, 11, 57434)}
2020-07-01 00:29:11 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:39:17 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:39:17 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:39:17 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:39:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:39:17 [scrapy.extensions.telnet] INFO: Telnet Password: da56adc8ec7a250a
2020-07-01 00:39:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:39:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:39:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:39:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:39:18 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:39:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:39:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:39:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:39:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:39:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:39:18 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:39:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:39:18 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:39:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.163516,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 39, 18, 423642),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 39, 18, 260126)}
2020-07-01 00:39:18 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:40:33 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:40:33 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:40:33 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:40:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:40:33 [scrapy.extensions.telnet] INFO: Telnet Password: 46bc0e1583e5bea0
2020-07-01 00:40:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:40:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:40:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:40:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:40:34 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:40:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:40:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:40:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:40:34 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:40:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.156574,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 40, 34, 835234),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 40, 34, 678660)}
2020-07-01 00:40:34 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:40:50 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:40:50 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:40:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:40:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:40:51 [scrapy.extensions.telnet] INFO: Telnet Password: 2fd721c786a9f154
2020-07-01 00:40:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:40:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:40:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:40:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:40:51 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:40:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:40:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:40:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:40:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:40:52 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:40:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.150591,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 40, 52, 92068),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 40, 51, 941477)}
2020-07-01 00:40:52 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:41:44 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:41:44 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:41:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:41:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:41:44 [scrapy.extensions.telnet] INFO: Telnet Password: aaee655883d5ef7d
2020-07-01 00:41:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:41:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:41:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:41:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:41:45 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:41:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:41:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:41:45 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:41:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:41:45 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:41:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.157579,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 41, 45, 812948),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 41, 45, 655369)}
2020-07-01 00:41:45 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:42:01 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:42:01 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:42:01 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:42:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:42:01 [scrapy.extensions.telnet] INFO: Telnet Password: eed400f9e44c36e5
2020-07-01 00:42:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:42:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:42:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:42:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:42:02 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:42:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:42:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:02 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:42:02 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:42:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.159603,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 42, 2, 790554),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 42, 2, 630951)}
2020-07-01 00:42:02 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:42:18 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:42:18 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:42:18 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:42:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:42:18 [scrapy.extensions.telnet] INFO: Telnet Password: 76cf06899c51c463
2020-07-01 00:42:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:42:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:42:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:42:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:42:19 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:42:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:42:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:42:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:42:19 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:42:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.148557,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 42, 19, 391108),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 42, 19, 242551)}
2020-07-01 00:42:19 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:43:42 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:43:42 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:43:42 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:43:42 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:43:42 [scrapy.extensions.telnet] INFO: Telnet Password: 75d06d66882dc15f
2020-07-01 00:43:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:43:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:43:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:43:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:43:43 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:43:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:43:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:43:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:43:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:43:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:43:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:43:43 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///D:/index.html> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    print(movie.xpath('./div[2]/a/div/div[2]/text()').extract_last().strip())
AttributeError: 'SelectorList' object has no attribute 'extract_last'
2020-07-01 00:43:44 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:43:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.234405,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 43, 44, 75595),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 43, 43, 841190)}
2020-07-01 00:43:44 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:48:32 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:48:32 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:48:32 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:48:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:48:32 [scrapy.extensions.telnet] INFO: Telnet Password: b6823cf1dbcb8ab0
2020-07-01 00:48:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:48:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:48:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:48:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:48:33 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:48:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:48:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:48:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:48:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:48:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:48:33 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:48:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:48:33 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:48:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.156609,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 48, 33, 315665),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 48, 33, 159056)}
2020-07-01 00:48:33 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:49:05 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:49:05 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:49:05 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:49:05 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:49:05 [scrapy.extensions.telnet] INFO: Telnet Password: ea1cd9b5dda5a0d1
2020-07-01 00:49:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:49:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:49:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:49:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-07-01 00:49:06 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:49:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:49:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:49:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:49:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:49:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:49:06 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:49:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:49:06 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:49:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.163575,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 49, 6, 494886),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 49, 6, 331311)}
2020-07-01 00:49:06 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:58:36 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:58:36 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:58:36 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:58:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:58:36 [scrapy.extensions.telnet] INFO: Telnet Password: c62c989868228b26
2020-07-01 00:58:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:58:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:58:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:58:37 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 00:58:37 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:58:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:58:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:58:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:58:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:58:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:58:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:58:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:58:37 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///D:/index.html> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    item.movie_name = movie.xpath('./div[2]/a/div/div[1]/span[1]/text()').extract_first()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\item.py", line 103, in __setattr__
    (name, value))
AttributeError: Use item['movie_name'] = '' to set field value
2020-07-01 00:58:37 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:58:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.141621,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 58, 37, 161700),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 58, 37, 20079)}
2020-07-01 00:58:37 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:59:14 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:59:14 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:59:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:59:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:59:14 [scrapy.extensions.telnet] INFO: Telnet Password: a95988550a97ff91
2020-07-01 00:59:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:59:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:59:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:59:15 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 00:59:15 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:59:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:59:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:59:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:59:15 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///D:/index.html> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 24, in parse
    item.movie_name = movie.xpath('./div[2]/a/div/div[1]/span[1]/text()').extract_first()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\item.py", line 103, in __setattr__
    (name, value))
AttributeError: Use item['movie_name'] = '' to set field value
2020-07-01 00:59:15 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:59:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.13963,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 59, 15, 280748),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 59, 15, 141118)}
2020-07-01 00:59:15 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 00:59:49 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 00:59:49 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 00:59:49 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 00:59:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 00:59:49 [scrapy.extensions.telnet] INFO: Telnet Password: a26aa140050a5280
2020-07-01 00:59:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 00:59:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 00:59:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 00:59:50 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 00:59:50 [scrapy.core.engine] INFO: Spider opened
2020-07-01 00:59:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 00:59:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 00:59:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:50 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 00:59:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 00:59:50 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///D:/index.html> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 25, in parse
    item.movie_name = movie.xpath('./div[2]/a/div/div[1]/span[1]/text()').extract_first().strip()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\item.py", line 103, in __setattr__
    (name, value))
AttributeError: Use item['movie_name'] = '' to set field value
2020-07-01 00:59:50 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 00:59:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.137597,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 16, 59, 50, 317967),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 16, 59, 50, 180370)}
2020-07-01 00:59:50 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:00:08 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:00:08 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:00:08 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:00:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:00:08 [scrapy.extensions.telnet] INFO: Telnet Password: cfea410f9b2a53e4
2020-07-01 01:00:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:00:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:00:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:00:09 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:00:09 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:00:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:00:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:00:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:09 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:00:09 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///D:/index.html> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 25, in parse
    item.movie_name = movie.xpath('./div[2]/a/div/div[1]/span[1]/text()').extract_first().strip()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\item.py", line 103, in __setattr__
    (name, value))
AttributeError: Use item['movie_name'] = '' to set field value
2020-07-01 01:00:09 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:00:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.139665,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 0, 9, 792899),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 0, 9, 653234)}
2020-07-01 01:00:09 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:00:26 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:00:26 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:00:26 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:00:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:00:26 [scrapy.extensions.telnet] INFO: Telnet Password: 9c6cad11fe3524f2
2020-07-01 01:00:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:00:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:00:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:00:27 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:00:27 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:00:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:00:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:00:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:00:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:00:27 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///D:/index.html> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 27, in parse
    item.movie_type = movie.xpath('./div[2]/a/div/div[2]/text()').extract()[1].strip()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\item.py", line 103, in __setattr__
    (name, value))
AttributeError: Use item['movie_type'] = '' to set field value
2020-07-01 01:00:27 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:00:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.138628,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 0, 27, 225266),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 0, 27, 86638)}
2020-07-01 01:00:27 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:03:20 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:03:20 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:03:20 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:03:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:03:20 [scrapy.extensions.telnet] INFO: Telnet Password: e924aa02d4cc754e
2020-07-01 01:03:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:03:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:03:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:03:21 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:03:21 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:03:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:03:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:03:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:03:21 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///D:/index.html> (referer: None)
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\spiders\maoyan.py", line 26, in parse
    item.movie_type = movie.xpath('./div[2]/a/div/div[2]/text()').extract()[1].strip()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\item.py", line 103, in __setattr__
    (name, value))
AttributeError: Use item['movie_type'] = '' to set field value
2020-07-01 01:03:21 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:03:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.142669,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 3, 21, 544852),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 3, 21, 402183)}
2020-07-01 01:03:21 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:03:51 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:03:51 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:03:51 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:03:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:03:51 [scrapy.extensions.telnet] INFO: Telnet Password: aa112b63f9df18d6
2020-07-01 01:03:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:03:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:03:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:03:52 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:03:52 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:03:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:03:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:03:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:03:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:03:52 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:03:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.188531,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 3, 52, 496026),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 3, 52, 307495)}
2020-07-01 01:03:52 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:04:10 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:04:10 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:04:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:04:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:04:10 [scrapy.extensions.telnet] INFO: Telnet Password: 4434cd53b73fcef3
2020-07-01 01:04:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:04:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:04:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:04:11 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:04:11 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:04:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:04:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:04:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:04:11 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:04:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.175482,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 4, 11, 594891),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 4, 11, 419409)}
2020-07-01 01:04:11 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:04:28 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:04:28 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:04:28 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:04:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:04:28 [scrapy.extensions.telnet] INFO: Telnet Password: ff6530880f11027f
2020-07-01 01:04:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:04:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:04:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:04:29 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:04:29 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:04:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:04:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:04:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:04:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:04:29 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:04:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.155555,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 4, 29, 732362),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 4, 29, 576807)}
2020-07-01 01:04:29 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:05:30 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:05:30 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:05:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:05:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:05:30 [scrapy.extensions.telnet] INFO: Telnet Password: 858d2d86d52f1c3e
2020-07-01 01:05:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:05:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:05:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:05:32 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:05:32 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:05:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:05:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:05:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:05:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:05:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:05:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:05:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:32 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:05:33 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:05:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.174545,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 5, 33, 14061),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 5, 32, 839516)}
2020-07-01 01:05:33 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:08:11 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:08:11 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:08:11 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:08:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:08:11 [scrapy.extensions.telnet] INFO: Telnet Password: 99ac9689c89bf232
2020-07-01 01:08:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:08:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:08:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:08:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:08:13 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:08:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:08:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:08:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:08:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:08:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:08:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:08:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:08:13 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:08:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.232378,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 8, 13, 472735),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 8, 13, 240357)}
2020-07-01 01:08:13 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:14:02 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:14:02 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:14:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:14:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:14:02 [scrapy.extensions.telnet] INFO: Telnet Password: d6008bb64bf52a97
2020-07-01 01:14:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:14:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:14:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:14:04 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:14:04 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:14:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:14:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:14:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:14:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:14:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:14:04 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:14:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:14:04 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:14:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.243349,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 14, 4, 834649),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 14, 4, 591300)}
2020-07-01 01:14:04 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:17:12 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:17:12 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:17:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:17:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:17:13 [scrapy.extensions.telnet] INFO: Telnet Password: dcd34389a6bc6dea
2020-07-01 01:17:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:17:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:17:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:17:14 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:17:14 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:17:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:17:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:14 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.scraper] ERROR: Error processing {'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\utils\defer.py", line 154, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\86151\learn_python\Python001-class01\week01\week01_homework_02\scrapy_movies\scrapy_movies\pipelines.py", line 13, in process_item
    movie_df.to_csv('top10_movies.csv', mode='a', encoding='GBK', index=False, header=True)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\core\generic.py", line 3204, in to_csv
    formatter.save()
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\formats\csvs.py", line 188, in save
    compression=dict(self.compression_args, method=self.compression),
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\common.py", line 428, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, newline="")
PermissionError: [Errno 13] Permission denied: 'top10_movies.csv'
2020-07-01 01:17:14 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:17:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.360006,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 17, 14, 977911),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 32,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 17, 14, 617905)}
2020-07-01 01:17:14 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:17:25 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:17:25 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:17:25 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:17:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:17:25 [scrapy.extensions.telnet] INFO: Telnet Password: 179af7a9d4b4a26e
2020-07-01 01:17:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:17:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:17:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:17:27 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:17:27 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:17:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:17:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:17:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:17:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:17:27 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:17:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.325102,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 17, 27, 904734),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 17, 27, 579632)}
2020-07-01 01:17:27 [scrapy.core.engine] INFO: Spider closed (finished)
2020-07-01 01:18:19 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapy_movies)
2020-07-01 01:18:19 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2020-07-01 01:18:19 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2020-07-01 01:18:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_movies',
 'LOG_FILE': 'all.log',
 'NEWSPIDER_MODULE': 'scrapy_movies.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_movies.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'}
2020-07-01 01:18:19 [scrapy.extensions.telnet] INFO: Telnet Password: f2d7d2ccb30a53ff
2020-07-01 01:18:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2020-07-01 01:18:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-07-01 01:18:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-07-01 01:18:21 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_movies.pipelines.ScrapyMoviesPipeline']
2020-07-01 01:18:21 [scrapy.core.engine] INFO: Spider opened
2020-07-01 01:18:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-07-01 01:18:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-07-01 01:18:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:18:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:18:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:18:21 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '\\robots.txt'
Traceback (most recent call last):
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\twisted\internet\defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "c:\users\86151\appdata\local\programs\python\python37-32\lib\site-packages\scrapy\core\downloader\handlers\file.py", line 13, in download_request
    with open(filepath, 'rb') as fo:
FileNotFoundError: [Errno 2] No such file or directory: '\\robots.txt'
2020-07-01 01:18:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///D:/index.html> (referer: None)
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-01', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2018-02-16', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-07-26', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-06', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-10-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-07-27', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-25', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-05-31',
 'movie_name': '2',
 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-10-20', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-04-24',
 'movie_name': '4',
 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-24', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-22', 'movie_name': '2', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-12-20',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-11-29', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-01-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2016-05-13', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-02-05', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2015-06-25', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-06-28', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-02-21',
 'movie_name': '',
 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2019-09-30', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2017-12-20', 'movie_name': '', 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///D:/index.html>
{'movie_date': '2020-03-06',
 'movie_name': ' CLIMAX',
 'movie_type': ''}
2020-07-01 01:18:21 [scrapy.core.engine] INFO: Closing spider (finished)
2020-07-01 01:18:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1163,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 90485,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.332076,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 6, 30, 17, 18, 21, 453209),
 'item_scraped_count': 30,
 'log_count/DEBUG': 33,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 "robotstxt/exception_count/<class 'FileNotFoundError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2020, 6, 30, 17, 18, 21, 121133)}
2020-07-01 01:18:21 [scrapy.core.engine] INFO: Spider closed (finished)
